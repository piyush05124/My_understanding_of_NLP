{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf9d08ec-0b30-4cf8-a058-129649a6d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb0d80fe-4849-4ada-849a-98b0f9941947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text  = '''Deep learning is a branch of machine learning that uses artificial neural networks to analyze data and make predictions. \n",
    "                It enables computers to learn from data, much like humans, by identifying patterns and features within the data. \n",
    "                Deep learning models are composed of multiple layers of interconnected nodes (neurons) that process information. \n",
    "                Introduction to Deep Learning'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd16b17-3765-436a-bd39-6a88ca6a09ad",
   "metadata": {},
   "source": [
    "# Using NLTK and word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a0096d1-0921-43fa-a1bb-f0f6dfb7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_corpus_by_words(sample_text,print_ = False ):\n",
    "    processed_corpus = []\n",
    "    text  = nltk.tokenize.sent_tokenize(sample_text) # converting given text corpus to sentences \n",
    "    stop_words = set(stopwords.words('english')) # set of stopwords in english language \n",
    "    for doc in text:\n",
    "        tokens = word_tokenize(doc.lower())\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "        processed_corpus.append(filtered_tokens)\n",
    "    if print_:\n",
    "        print(\"Processed Corpus:\")\n",
    "        for doc in processed_corpus:\n",
    "            print(doc)\n",
    "    return processed_corpus\n",
    "\n",
    "processed_corpus = filter_corpus_by_words(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86622a1-e789-43da-900b-5436cc83eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = Word2Vec(sentences=processed_corpus, vector_size=20, min_count=1, sg=0,compute_loss=True,epochs=82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8ac68c-e3b6-43d5-a688-59498367c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word2Vec embedding for 'learning':\n",
      "[-0.0041298   0.00052904  0.02738201  0.04280639 -0.04486082 -0.03578677\n",
      "  0.03607799  0.04926915 -0.02861241 -0.01850656  0.04098072 -0.0094584\n",
      " -0.02237132  0.03266828 -0.02282948 -0.00871675  0.01877594  0.00700811\n",
      " -0.04272503 -0.04905445]\n",
      "Shape: (20,)\n",
      "\n",
      "Words similar to 'learning':\n",
      "[('process', 0.5181392431259155), ('humans', 0.49196934700012207), ('machine', 0.4343571960926056), ('interconnected', 0.39095520973205566), ('enables', 0.37636977434158325)]\n"
     ]
    }
   ],
   "source": [
    "word = 'learning'\n",
    "if word in vectors.wv:\n",
    "    word_vector = vectors.wv[word]\n",
    "    print(f\"\\nWord2Vec embedding for '{word}':\\n{word_vector}\")\n",
    "    print(f\"Shape: {word_vector.shape}\")\n",
    "\n",
    "    # 4. Find similar words\n",
    "    similar_words = vectors.wv.most_similar(word, topn=5)\n",
    "    print(f\"\\nWords similar to '{word}':\\n{similar_words}\")\n",
    "else:\n",
    "    print(f\"\\nWord '{word}' not in Word2Vec vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1911e30-980d-4137-929b-9b16a5acf411",
   "metadata": {},
   "source": [
    "# Using tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22420c73-2655-4f2e-9e0c-962aaa0cec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f68356b-0a31-45a5-9141-2a2623737288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Deep learning is a branch of machine learning that uses artificial neural networks to analyze data and make predictions.', 'It enables computers to learn from data, much like humans, by identifying patterns and features within the data.', 'Deep learning models are composed of multiple layers of interconnected nodes (neurons) that process information.', 'Introduction to Deep Learning']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[2, 1, 8, 9, 10, 3, 11, 1, 6, 12, 13, 14, 15, 4, 16, 5, 7, 17, 18],\n",
       " [19, 20, 21, 4, 22, 23, 5, 24, 25, 26, 27, 28, 29, 7, 30, 31, 32, 5],\n",
       " [2, 1, 33, 34, 35, 3, 36, 37, 3, 38, 39, 40, 6, 41, 42],\n",
       " [43, 4, 2, 1]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "texts  = nltk.tokenize.sent_tokenize(sample_text)\n",
    "print(texts)\n",
    "# 2. Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=100,lower=True) # Consider top 100 words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d91145fb-9db6-4c74-afbb-983af69eb4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Index (Tensorflow Tokenizer):\n",
      "{'learning': 1, 'deep': 2, 'of': 3, 'to': 4, 'data': 5, 'that': 6, 'and': 7, 'is': 8, 'a': 9, 'branch': 10, 'machine': 11, 'uses': 12, 'artificial': 13, 'neural': 14, 'networks': 15, 'analyze': 16, 'make': 17, 'predictions': 18, 'it': 19, 'enables': 20, 'computers': 21, 'learn': 22, 'from': 23, 'much': 24, 'like': 25, 'humans': 26, 'by': 27, 'identifying': 28, 'patterns': 29, 'features': 30, 'within': 31, 'the': 32, 'models': 33, 'are': 34, 'composed': 35, 'multiple': 36, 'layers': 37, 'interconnected': 38, 'nodes': 39, 'neurons': 40, 'process': 41, 'information': 42, 'introduction': 43}\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(f\"\\nWord Index (Tensorflow Tokenizer):\\n{word_index}\")\n",
    "# print(f\"Sequences:\\n{sequences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36faad6-ac94-4cb6-b8d2-f28444b2e8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Padded Sequences with max len:19\n",
      "[[ 2  1  8  9 10  3 11  1  6 12 13 14 15  4 16  5  7 17 18]\n",
      " [19 20 21  4 22 23  5 24 25 26 27 28 29  7 30 31 32  5  0]\n",
      " [ 2  1 33 34 35  3 36 37  3 38 39 40  6 41 42  0  0  0  0]\n",
      " [43  4  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Pad sequences to ensure uniform length\n",
    "max_sequence_len = max(len(s) for s in sequences)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_len, padding='post')\n",
    "print(f\"\\nPadded Sequences with max len:{max_sequence_len}\\n{padded_sequences}\")\n",
    "\n",
    "\n",
    "\n",
    "# padded_sequences is final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57a364d-d36e-4565-8a80-71db4b61dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word_index) + 1  # Add 1 for the padding token (0)\n",
    "print(vocab_size)\n",
    "embedding_dim = 64 # Desired dimensionality of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd77274-8299-48c0-b2f0-db59300a8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output of Keras Embedding Layer (shape): (4, 19, 64)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                            output_dim=embedding_dim,\n",
    "                            input_length=max_sequence_len)\n",
    "\n",
    "\n",
    "embedded_sequences = embedding_layer(padded_sequences)\n",
    "\n",
    "print(f\"\\nOutput of Keras Embedding Layer (shape): {embedded_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b6a4e4c-aea7-415d-9a69-c4c96597a3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example embedding for the first word of the first sentence:\n",
      "[-0.03097786 -0.01355761 -0.03380464  0.04986094  0.03843668 -0.03074837\n",
      "  0.0048359   0.04037979  0.03339995 -0.02191104 -0.0089682  -0.00721304\n",
      "  0.02046641  0.01346039 -0.01451672  0.01834874 -0.02754556 -0.01184278\n",
      " -0.03647103 -0.00761991 -0.00952339 -0.03455668 -0.02673699 -0.03792853\n",
      "  0.04597051 -0.01492751 -0.03240017  0.03820202  0.04928113 -0.04756137\n",
      "  0.04092007  0.01362092  0.01053228 -0.04305983  0.04813161 -0.01688687\n",
      "  0.04349688  0.03210661 -0.01295843 -0.03543295  0.0239558   0.03186109\n",
      " -0.04616003  0.00829976  0.00840474  0.00268687  0.00433765  0.01358309\n",
      "  0.03977385 -0.01771982  0.00192568  0.03902122  0.0350115   0.04065806\n",
      "  0.02825171  0.0483082   0.02349967  0.01741841  0.01136048 -0.00195349\n",
      " -0.03629371  0.0294208   0.02775467  0.0027809 ]\n"
     ]
    }
   ],
   "source": [
    "# (batch_size, sequence_length, embedding_dim)\n",
    "# batch_size is the number of texts (4 in this case)\n",
    "# sequence_length is max_sequence_len (19)\n",
    "# embedding_dim is 16\n",
    "print(f\"Example embedding for the first word of the first sentence:\\n{embedded_sequences[0, 0, :]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c654d-0be8-4995-86a7-6813cab756fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
